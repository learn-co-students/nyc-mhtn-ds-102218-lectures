{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining and NLP\n",
    "\n",
    "## Part 2\n",
    "\n",
    "### Situation:\n",
    "\n",
    "Priya works at an international PR firm in the Europe division. Their largest client has offices in Ibiza, Madrid, and Las Palmas. She needs to keep her boss aware of current events and provide a weekly short list of articles concerning political events in Spain. The problem is, this takes hours every week to review articles on the BBC and Priya is very busy! She wonders if she could automate this process using text mining to save her time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Goal**: to internalize the steps, challenges, and methodology of text mining\n",
    "- explore text analysis by hand\n",
    "- apply text mining steps in Jupyter with Python libraries NLTK\n",
    "- classify documents correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher on cleaning text\n",
    "![gif](https://www.nyfa.edu/student-resources/wp-content/uploads/2014/10/furious-crazed-typing.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from __future__ import print_function\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "url_a = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/A.txt\"\n",
    "url_b = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/B.txt\"\n",
    "article_a = urllib.request.urlopen(url_a).read()\n",
    "article_a_st = article_a.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "arta_tokens_raw = nltk.regexp_tokenize(article_a_st, pattern)\n",
    "\n",
    "# lower case\n",
    "arta_tokens = [i.lower() for i in arta_tokens_raw]\n",
    "\n",
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "arta_tokens_stopped = [w for w in arta_tokens if not w in stop_words]\n",
    "\n",
    "# stem words\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "arta_stemmed = [stemmer.stem(word) for word in arta_tokens_stopped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat w second article\n",
    "article_b = urllib.request.urlopen(url_b).read()\n",
    "article_b_st = article_b.decode(\"utf-8\")\n",
    "artb_tokens_raw = nltk.regexp_tokenize(article_b_st, pattern)\n",
    "artb_tokens = [i.lower() for i in artb_tokens_raw]\n",
    "artb_tokens_stopped = [w for w in artb_tokens if not w in stop_words]\n",
    "artb_stemmed = [stemmer.stem(word) for word in artb_tokens_stopped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what's wrong with the table from yesterday? what does it not consider?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency (DF)\n",
    "\n",
    "$\\begin{align}\n",
    " tf_{i,j} = \\dfrac{n_{i,j}}{\\displaystyle \\sum_k n_{i,j} }\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF-IDF score\n",
    "\n",
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The from scratch method\n",
    "![homemade](https://media2.giphy.com/media/LBZcXdG0eVBdK/giphy.gif?cid=3640f6095c2d7bb2526a424a4d97117c)\n",
    "\n",
    "\n",
    "Please go through the code and comment what each section does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet = set(arta_stemmed).union(set(artb_stemmed))\n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0) \n",
    "\n",
    "for word in arta_stemmed:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in artb_stemmed:\n",
    "    wordDictB[word]+=1    \n",
    "\n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "tfbowA = computeTF(wordDictA,arta_stemmed)\n",
    "tfbowB = computeTF(wordDictB,artb_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfbowA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBowA = computeTFIDF(tfbowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfbowB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But yes, there is an easier way\n",
    "\n",
    "![big deal](https://media0.giphy.com/media/xUA7aQOxkz00lvCAOQ/giphy.gif?cid=3640f6095c2d7c51772f47644d09cc8b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a string again\n",
    "cleaned_a = ' '.join(arta_stemmed)\n",
    "cleaned_b = ' '.join(artb_stemmed)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform([cleaned_a, cleaned_b])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics \n",
    "\n",
    "How many non-zero elements are there?\n",
    "- Adapt the code below, using the `df` version of the `response` object to replace everywhere below it says `DATA`\n",
    "- Interpret the findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit code before running it\n",
    "\n",
    "non_zero_cols = DATA.nnz / float(DATA.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(DATA.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "- Create the tf-idf for the **whole** corpus of 12 articles\n",
    "- What are _on average_ the most important words in the whole corpus?\n",
    "- Add a column named \"Target\" to the dataset\n",
    "- Target will be set to 1 or 0 if the article is \"Politics\" or \"Not Politics\"\n",
    "- Do some exploratory analysis of the dataset\n",
    " - what are the average most important words for the \"Politics\" articles?\n",
    " - What are the average most important words for the \"Not Politics\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets talk classification\n",
    "- How would you split into train and test? what would be the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
